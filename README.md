# Water Supply Forecast Rodeo

![Python 3.10](https://img.shields.io/badge/Python-3.10-blue) [![DrivenData Water Supply Forecast Rodeo](https://img.shields.io/badge/DrivenData-Water%20Supply%20Forecast%20Rodeo-white?logo=data:image/png;base64,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)](https://watersupply.drivendata.org/)

Welcome to the data and runtime repository the [Water Supply Forecast Rodeo](https://watersupply.drivendata.org/) competition on DrivenData! This repository contains a few things:

1. **Data download code** ([`data_download/`](./data_download/)) â€” a Python package with a code and a CLI for downloading data for each [approved feature data source](https://www.drivendata.org/competitions/254/reclamation-water-supply-forecast-dev/page/801/). DrivenData will download datasets from certain approved data sources and mount it to the competition runtime for code execution submissions. Use the CLI to reproduce the saved file structure in the runtime.
2. **Data reading code** ([`data_reading/`](./data_reading/)) â€” a Python library with example code for loading each of the feature datasets downloaded by the data download package, available for you to optionally use. It will be installed in the code execution runtime environment and you will be able to import it.
3. **Submission template** ([`examples/template/`](./examples/template/solution.py)) â€” a template with the function signatures that you should implement in your submission
3. **Example submission** ([`examples/moving_average/`](./examples/moving_average/)) â€” a submission with a simple demonstration solution. It will run successfully in the code execution runtime and outputs a valid submission.
4. **Runtime environment specification** ([`runtime/`](./runtime/)) â€” the definition of the environment where your code will run.

You can use this repository to:

â¬‡ï¸ **Get feature data**: The same code that is used to get feature data for the runtime environment is available for you to use locally.

ðŸ”§ **Test your submission**: Test your submission using a locally running version of the competition runtime to discover errors before submitting to the competition website.

ðŸ“¦ **Request new packages in the official runtime**: Since your submission will not have general access to the internet, all dependencies must be pre-installed. If you want to use a package that is not in the runtime environment, make a pull request to this repository. Make sure to test out adding the new package to both official environments, CPU and GPU.

Changes to the repository are documented in [CHANGELOG.md](./CHANGELOG.md).

---

#### [1. Data download](#data-download)

- [Requirements and installation](#requirements-and-installation)
- [Usage](#usage)
- [Details](#details)

#### [2. Data reading](#data-reading)

- [Requirements and installation](#requirements-and-installation-1)
- [Usage](#usage-1)

#### [3. Testing a submission locally](#testing-a-submission-locally)

- [Prerequesites](#prerequesites)
- [Setting up the data directory](#setting-up-the-data-directory)
- [Code submission format](#code-submission-format)
- [Running your submission locally](#running-your-submission-locally)
- [Smoke tests](#smoke-tests)
- [Runtime network access](#runtime-network-access)

#### [4. Updating runtime packages](#updating-runtime-packages)

#### [5. Makefile commands](#makefile-commands)

---

## Data download

This repo contains a Python package named `wsfr-download` located in the `data_download/` directory. It provides a command-line interface (CLI) for downloading approved challenge datasets. DrivenData will use this package to download the test feature data that will be made available to the code execution runtime. You can use it to download feature data in the same way for testing your submission or for training.

> [!NOTE]
> Data download code may be added for requested data sources that get approved.

### Requirements and installation

Requires Python 3.10. To install with the exact dependencies that will be used by DrivenData, create a new virtual environment and run

```bash
pip install -r ./data_download/requirements.txt
pip install ./data_download/
```

By default, data is saved into a subdirectory named `data/` relative to your current working directory. You can explicitly override this by setting the environment variable `WSFR_DATA_ROOT` with another directory path. The expected default usage is that you run all commands with the root directory of this repository as your working directory.

You will also need to download the following files from the competition [data download page](https://www.drivendata.org/competitions/254/reclamation-water-supply-forecast-dev/data/) and place them into your data directory. The data download scripts will depend on some of these files.

- `geospatial.gpkg` -> `data/geospatial.gpkg`
- `metadata.csv` -> `data/metadata.csv`
- `cdec_snow_stations.csv` -> `data/cdec_snow_stations.csv`
- `cpc_climate_divisions.gpkg` -> `data/cpc_climate_divisions.gpkg`
- `nlcd_release_dates.csv` -> `data/nlcd_release_dates.csv`

Additionally, the following data products are static releases and involve large single files. If you are planning to use any of the following datasets, please manually download it from its approved source and move it into the designated location.

- [BasinATLAS basin attributes](https://www.drivendata.org/competitions/254/reclamation-water-supply-forecast-dev/page/801/#basinatlas-basin-attributes) -> `data/BasinATLAS_Data_v10.gdb.zip` (2.7 GB)
- [NLCD Urban Imperviousness](https://www.drivendata.org/competitions/254/reclamation-water-supply-forecast-dev/page/801/#national-land-cover-database-nlcd-urban-imperviousness) -> `data/NLCD_impervious_2021_release_all_files_20230630.zip` (19.54 GB)

You will need at least 115 GB in free disk space to download all datasets. See the ["Expected files"](#expected-files) section below for a breakdown by data source.

### Usage

To simply download all test feature data that will be available, use the `bulk` command. From the repository root as your working directory, run:

```bash
python -m wsfr_download bulk data_download/hindcast_test_config.yml
```

#### Details

You can invoke the CLI with `python -m wsfr_download`. For example, to see a list of all available commands:

```bash
python -m wsfr_download --help
```

The CLI is organized with one command per data source, e.g., see

```bash
python -m wsfr_download grace_indicators --help
```

There is also the `bulk` command for downloading multiple data sources at once, as shown in the previous section. A bulk download is configured by a YAML configuration file. The configuration file for the Hindcast test set is [`data_download/hindcast_test_config.yml`](data_download/hindcast_test_config.yml). To download feature data for training, create your own YAML configuration file for the years and data sources that you need using the test set file as an example.

By default, all download functions will skip downloading data for files that already exist in your data directory. This is controlled by an option called `skip_existing`. To force downloads to overwrite existing files, set `skip_existing` to `false` in the bulk download config file when using the `bulk` command, or use the `--no-skip-existing` flag when using an individual data source's download command.

### Expected files

A list of all files present in the runtime data volume is available in [`data.find.txt`](./data.find.txt). You can generate an equivalent version of this file for your local data directory with the following command:

```sh
find data -type f ! -name '.DS_Store' ! -name '.gitkeep' | sort
```

You can also find a listing of subdirectory sizes in [`data.du.txt`](./data.du.txt), which will give you an idea of the disk space needed for each data source. You can generate an equivalent version of this file for your local data directory with the following command:

```sh
du -sh data/*
```

## Data reading

This repo contains a Python package named `wsfr-read` located in the `data_reading/` directory. It provides a library with example functions to read the data downloaded by `wsfr-download`. This package will be installed into the code execution runtime for you to optionally use during inference on the test set. These functions may be helpful because they implement subsetting by `site_id` and `issue_date`. You are not required to use these functions in your solution.

> [!NOTE]
> Data reading code may be added for requested data sources that get approved.

### Requirements and installation

Requires Python 3.10. Install with pip:

```bash
pip install ./data_reading/
```

### Usage

Modules are provided with names matching the data source names in the `wsfr-download` package. Each module contains `read_*_data` functions that are basic ways you can load that data for use as features for your models. See the docstrings on the functions for more details on usage.

By default, data is assumed to be in a subdirectory named `data/` relative to your current working directory. You can explicitly override this by setting the environment variable `WSFR_DATA_ROOT`.

## Testing a submission locally

When you make a submission on the DrivenData competition site, we run your submission inside a Docker container, a virtual operating system that allows for a consistent software environment across machines. **The best way to make sure your submission to the site will run is to first run it successfully in the container on your local machine.**

### Prerequesites

- A clone of this repository
- [Docker](https://docs.docker.com/get-docker/)
- At least 5 GB of free space for the CPU version of the Docker image or at least 10 GB of free space for the GPU version
- [GNU make](https://www.gnu.org/software/make/) (optional, but useful for running the commands in the Makefile)

Additional requirements to run with GPU:

- [NVIDIA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-installation) with **CUDA 11**
- [NVIDIA container toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/index.html)

### Setting up the data directory

In the official code execution platform, `code_execution/data` will contain data provided for the test set. This will include data from the [data download page](https://www.drivendata.org/competitions/257/reclamation-water-supply-forecast-hindcast/data/) as well as feature data downloaded by the data pipelines in `data_download/`. See the [data download section](#data-download) for more about setting up the test data.

In additional to the files detailed in the data download section, you will also need the following additional two files from the [data download page](https://www.drivendata.org/competitions/257/reclamation-water-supply-forecast-hindcast/data/):

- `submission_format.csv` -> `data/submission_format.csv`
- `smoke_submission_format.csv` -> `data/smoke_submission_format.csv`

When testing your submission locally, the `data/` directory in the repository root will be mounted into the container. You can explicitly override this by setting the environment variable `WSFR_DATA_ROOT` with another directory path.

### Code submission format

Your final submission should be a zip archive named with the extension `.zip` (for example, `submission.zip`). The root level of the `submission.zip` file must contain a `solution.py` which contains a `predict` function that returns predictions for a single site on a single issue date.

A template for `solution.py` is included at [`examples/template/solution.py`](./examples/template/solution.py). For more detail, see the "what to submit" section of the [code submission page](https://www.drivendata.org/competitions/257/reclamation-water-supply-forecast-hindcast/page/809/#what-to-submit).

### Running your submission locally

This section provides instructions on how to run the your submission in the code execution container from your local machine. To simplify the steps, key processes have been defined in the `Makefile`. Commands from the `Makefile` are then run with `make {command_name}`. The basic steps are:

```sh
make pull
make pack-submission
make test-submission
```

Run `make help` for more information about the available commands as well as information on the official and built images that are available locally.

Here's the process in a bit more detail:

1. First, make sure you have set up the [prerequisites](#prerequisites).
2. Download the official competition Docker image:

    ```sh
    make pull
    ```

> [!NOTE]
> If you have built a local version of the runtime image with `make build`, that image will take precedence over the pulled image when using any make commands that run a container. You can explicitly use the pulled image by setting the `SUBMISSION_IMAGE` shell/environment variable to the pulled image or by deleting all locally built images.

3. Save all of your submission files, including the required `solution.py` script, in the `submission_src` folder of the runtime repository. Make sure any needed model weights and other assets are saved in `submission_src` as well.

4. Create a `submission/submission.zip` file containing your code and model assets:

    ```sh
    make pack-submission
    #> mkdir -p submission/
    #> cd submission_src; zip -r ../submission/submission.zip ./*
    #>   adding: solution.py (deflated 73%)
    ```

5. Launch an instance of the competition Docker image, and run the same inference process that will take place in the official runtime:

    ```sh
    make test-submission
    ```

This runs the container [entrypoint](./runtime/entrypoint.sh) script. First, it unzips `submission/submission.zip` into `/code_execution/src/` in the container. Then, it runs the [`supervisor.py`](./runtime/supervisor.py) script, which will import code from your submitted `solution.py`. In the local testing setting, the final submission is saved out to `submission/submission.csv` on your local machine.

When you run `make test-submission` the logs will be printed to the terminal and written out to `submission/log.txt`. If you run into errors, use the `log.txt` to determine what changes you need to make for your code to execute successfully.

### Example submission

An example code submission is provided in [`examples/moving_average`](./examples/moving_average/) that can run successfully and generate valid predictions. Please note that this model is not a realistic solution to the problem. You can use the example in place of steps 3 and 4 above. To pack this submission for testing or for submission to the platform, run:

```sh
make pack-example
```

### Smoke tests

When submitting on the platform, you will have the ability to submit "smoke tests". Smoke tests run on a reduced version of the test set in order to run more quickly. They will not be considered for prize evaluation and are intended to let you test your code for correctness.

Smoke tests use the `smoke_submission_format.csv` file instead of the full `submission_format.csv` file. When testing locally, a submission will run as a smoke test if the `IS_SMOKE` shell variable is set to a non-empty string. For example,

```sh
IS_SMOKE=1 make test-submission
```

You can read more about smoke tests on the [code submission format page](https://www.drivendata.org/competitions/257/reclamation-water-supply-forecast-hindcast/page/809/#smoke-tests).

### Runtime network access

In the real competition runtime, all internet access is blocked except to the hosts documented in [`allowed_hosts.txt`](./allowed_hosts.txt) corresponding to the approved data sources labeled with "Direct API access permitted" on the [Approved data sources page](https://www.drivendata.org/competitions/254/reclamation-water-supply-forecast-dev/page/801/).

The local test runtime does not impose any network restrictions; as a result **submissions that require internet access might succeed in local tests but fail in the actual competition runtime.** It's up to you to make sure that your code does not make requests to unauthorized web resources. If your submission does not require internet access, you can test your submission _without_ internet access by running `BLOCK_INTERNET=true make test-submission`.

## Updating runtime packages

If you want to use a package that is not in the environment, you are welcome to make a pull request to this repository. If you're new to the GitHub contribution workflow, check out [this guide by GitHub](https://docs.github.com/en/get-started/quickstart/contributing-to-projects).

The runtime manages dependencies using [conda](https://docs.conda.io/en/latest/) environments and [conda-lock](https://github.com/conda/conda-lock). [Here is a good general guide](https://towardsdatascience.com/a-guide-to-conda-environments-bc6180fc533) to conda environments. The official runtime uses **Python 3.10.13** environments.

To submit a pull request for a new package:

1. Fork this repository.

2. Install conda-lock. See [here](https://github.com/conda/conda-lock#installation) for installation options.

3. Edit the [conda](https://docs.conda.io/en/latest/) environment YAML files, `runtime/environment-cpu.yml` and `runtime/environment-gpu.yml`. There are two ways to add a requirement:

    - Conda package manager **(preferred)**: Add an entry to the `dependencies` section. This installs from the [conda-forge](https://anaconda.org/conda-forge/) channel using `conda install`. Conda performs robust dependency resolution with other packages in the `dependencies` section, so we can avoid package version conflicts.
    - Pip package manager: Add an entry to the `pip` section. This installs from PyPI using `pip`, and is an option for packages that are not available in a conda channel.

4. Run `make update-lockfiles`. This will read `environment-cpu.yml` and `environment-gpu.yml`, resolve exact package versions, and save the pinned environments to `conda-lock-cpu.yml` and `conda-lock-gpu.yml`.

5. Locally test that the Docker image builds successfully for CPU and GPU images:

    ```sh
    CPU_OR_GPU=cpu make build
    CPU_OR_GPU=gpu make build
    ```

6. Commit the changes to your forked repository. Ensure that your branch includes updated versions of _all_ of the following:

    - `runtime/conda-lock-cpu.yml`
    - `runtime/conda-lock-gpu.yml`
    - `runtime/environment-cpu.lock`
    - `runtime/environment-cpu.yml`
    - `runtime/environment-gpu.lock`
    - `runtime/environment-gpu.yml`

7. Open a pull request from your branch to the `main` branch of this repository. Navigate to the [Pull requests](https://github.com/drivendataorg/water-supply-forecast-rodeo-runtime/pulls) tab in this repository, and click the "New pull request" button. For more detailed instructions, check out [GitHub's help page](https://help.github.com/en/articles/creating-a-pull-request-from-a-fork).

8. Once you open the pull request, we will use Github Actions to build the Docker images with your changes and run the tests in `runtime/tests`. For security reasons, administrators may need to approve the workflow run before it happens. Once it starts, the process can take up to 30 minutes, and may take longer if your build is queued behind others. You will see a section on the pull request page that shows the status of the tests and links to the logs.

9. You may be asked to submit revisions to your pull request if the tests fail or if a DrivenData staff member has feedback. Pull requests won't be merged until all tests pass and the team has reviewed and approved the changes.

## Make commands

A Makefile with several helpful shell recipes is included in the repository. The runtime documentation above uses it extensively. Running `make` by itself in your shell will list relevant Docker images and provide you the following list of available commands:

```
Available commands:

build               Builds the container locally
clean               Delete temporary Python cache and bytecode files
interact-container  Open an interactive bash shell within the running container (with network access)
pack-example        Creates a submission/submission.zip file from the source code in examples_src
pack-submission     Creates a submission/submission.zip file from the source code in submission_src
pull                Pulls the official container from Azure Container Registry
test-container      Ensures that your locally built image can import all the Python packages successfully when it runs
test-submission     Runs container using code from `submission/submission.zip` and data from WSFR_DATA_ROOT (default `data/`)
update-lockfiles    Updates runtime environment lockfiles
```
